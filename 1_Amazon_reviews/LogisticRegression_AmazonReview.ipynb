{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression on Amazon review data  \n",
    "The goal of this project is to predict whether the sentiments about a product (from its reviews) are positive or negative using product review data from Amazon.\n",
    "\n",
    "The review data is stored in the form of SFrame.\n",
    "\n",
    "\n",
    "#### Skills learned:\n",
    "- extract bag-of-word features with CountVectorizer in scikit learn\n",
    "- compare two classification models: classify with the sign of score or a logit link\n",
    "- compare the effect of vocabolary size on classifying Amazon review data\n",
    "- benchmarked with majority-class classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading and cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] sframe.cython.cy_server: SFrame v2.1 started. Logging /tmp/sframe_server_1475025929.log\n",
      "INFO:sframe.cython.cy_server:SFrame v2.1 started. Logging /tmp/sframe_server_1475025929.log\n"
     ]
    }
   ],
   "source": [
    "import sframe\n",
    "# from graphlab import SFrame\n",
    "products = sframe.SFrame('amazon_baby.gl/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this is suppose to work but not working :(\n",
    "# let's stick with sframe for now...\n",
    "# products_df = SFrame.to_dataframe(products)\n",
    "# Apparently I need graphlab in order to output sframe into dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's strip off the punctuation first:\n",
    "def remove_punctuation(text):\n",
    "    import string\n",
    "    return text.translate(None, string.punctuation) \n",
    "\n",
    "products['review_clean'] = products['review'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMPORTANT. \n",
    "Make sure to fill n/a values in the review column with empty strings (if applicable). The n/a values indicate empty reviews. \n",
    "(In pandas, the syntax will be: products = products.fillna({'review':''}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "products = products.fillna('review','')  # fill in N/A's in the review column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# As required by the assignment, let's ignore all the neutral ratings.\n",
    "\n",
    "products = products[products['rating'] != 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's create a new binary prediction for positive/negative experience\n",
    "products['sentiment'] = products['rating'].apply(lambda rating : +1 if rating > 3 else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the training and testing sets\n",
    "train_data, test_data = products.random_split(.8, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype: int\n",
       "Rows: 166752\n",
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, -1, 1, -1, 1, 1, 1, -1, -1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ... ]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction \n",
    "#### bag-of-words with scikit learn\n",
    "Let's use sparse matrices to store the word counts. \n",
    "\n",
    "General advise from the instructors:\n",
    "- Learn a vocabulary (set of all words) from the training data. Only the words that show up in the training data will be considered for feature extraction.\n",
    "- Compute the occurrences of the words in each review and collect them into a row vector.\n",
    "- Build a sparse matrix where each row is the word count vector for the corresponding review. Call this matrix train_matrix.\n",
    "- Using the same mapping between words and columns, convert the test data into a sparse matrix test_matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Use this token pattern to keep single-letter words\n",
    "# First, learn vocabulary from the training data and assign columns to words\n",
    "vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b')\n",
    "\n",
    "# Then convert the training data into a sparse matrix\n",
    "train_matrix = vectorizer.fit_transform(train_data['review_clean'])\n",
    "# Second, convert the test data into a sparse matrix, using the same word-column mapping\n",
    "test_matrix = vectorizer.transform(test_data['review_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<133416x121712 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 7326618 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training\n",
    "7. Learn a logistic regression classifier using the training data. If you are using scikit-learn, you should create an instance of the LogisticRegression class and then call the method fit() to train the classifier. This model should use the sparse word count matrix (train_matrix) as features and the column sentiment of train_data as the target. Use the default values for other parameters. Call this model sentiment_model.\n",
    "\n",
    "8. There should be over 100,000 coefficients in this sentiment_model. Recall from the lecture that positive weights w_j correspond to weights that cause positive sentiment, while negative weights correspond to negative sentiment. Calculate the number of positive (>= 0, which is actually nonnegative) coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "sentiment_model = LogisticRegression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentiment_model = sentiment_model.fit(train_matrix, train_data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(133416, 121712)\n",
      "(133416,)\n"
     ]
    }
   ],
   "source": [
    "print train_matrix.shape\n",
    "print train_data['sentiment'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96850452719314029"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print model accuracy\n",
    "sentiment_model.score(train_matrix, train_data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87059\n"
     ]
    }
   ],
   "source": [
    "#Quiz question: How many weights are >= 0?\n",
    "import numpy as np\n",
    "print np.sum(sentiment_model.coef_>=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's first look at properties of three specific entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+-------------------------------+--------+\n",
      "|              name             |             review            | rating |\n",
      "+-------------------------------+-------------------------------+--------+\n",
      "|   Our Baby Girl Memory Book   | Absolutely love it and all... |  5.0   |\n",
      "| Wall Decor Removable Decal... | Would not purchase again o... |  2.0   |\n",
      "| New Style Trailing Cherry ... | Was so excited to get this... |  1.0   |\n",
      "+-------------------------------+-------------------------------+--------+\n",
      "+-------------------------------+-----------+\n",
      "|          review_clean         | sentiment |\n",
      "+-------------------------------+-----------+\n",
      "| Absolutely love it and all... |     1     |\n",
      "| Would not purchase again o... |     -1    |\n",
      "| Was so excited to get this... |     -1    |\n",
      "+-------------------------------+-----------+\n",
      "[3 rows x 5 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sample_test_data = test_data[10:13]\n",
    "print sample_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Absolutely love it and all of the Scripture in it.  I purchased the Baby Boy version for my grandson when he was born and my daughter-in-law was thrilled to receive the same book again.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_test_data[0]['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Would not purchase again or recommend. The decals were thick almost plastic like and were coming off the wall as I was applying them! The would NOT stick! Literally stayed stuck for about 5 minutes then started peeling off.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_test_data[1]['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Now let's use the trained model to make some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  5.60150644  -3.17110494 -10.42378277]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sample_test_matrix = vectorizer.transform(sample_test_data['review_clean'])\n",
    "scores = sentiment_model.decision_function(sample_test_matrix)\n",
    "print scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "If we predict with the sign of the score, the prediction would be 1, -1, -1.\n",
    "In order to quantify the degree of confidence in our predictions, let's use a logic link -P(yi=1|xi,w)=1/(1+exp(-wT h(x))) -  and translate the scores into probabilities of a comment being position or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.99632128559196442, 0.04026769205939245, 2.9716370056769632e-05]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1/(1+np.exp(-s)) for s in scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, -1, -1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's compare our calculation with the prediction from the trained model\n",
    "sentiment_model.predict(sample_test_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "Using the sentiment_model, find the 20 reviews in the entire test_data with the highest probability of being classified as a positive review. We refer to these as the \"most positive reviews.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_matrix = vectorizer.transform(test_data['review_clean'])\n",
    "total_scores = sentiment_model.decision_function(test_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prob = [1/(1+np.exp(-s)) for s in total_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data['prob'] = prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we need to sort the SFrame based on 'prob'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sframe.data_structures.sframe.SFrame"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+-------------------------------+--------+\n",
      "|              name             |             review            | rating |\n",
      "+-------------------------------+-------------------------------+--------+\n",
      "| Freemie Hands-Free Conceal... | I absolutely love this pro... |  5.0   |\n",
      "| Baby Einstein Around The W... | I am so HAPPY I brought th... |  5.0   |\n",
      "| Fisher-Price Cradle 'N Swi... | My husband and I cannot st... |  5.0   |\n",
      "| P'Kolino Silly Soft Seatin... | I've purchased both the P'... |  4.0   |\n",
      "| Buttons Cloth Diaper Cover... | We are big Best Bottoms fa... |  4.0   |\n",
      "| Baby Jogger City Mini GT S... | Amazing, Love, Love, Love ... |  5.0   |\n",
      "| Mamas &amp; Papas 2014 Urb... | After much research I purc... |  4.0   |\n",
      "| Britax Decathlon Convertib... | I researched a few differe... |  4.0   |\n",
      "| Roan Rocco Classic Pram St... | Great Pram Rocco!!!!!!I bo... |  5.0   |\n",
      "| Simple Wishes Hands-Free B... | I just tried this hands fr... |  5.0   |\n",
      "| Graco Pack 'n Play Element... | My husband and I assembled... |  4.0   |\n",
      "| Diono RadianRXT Convertibl... | I bought this seat for my ... |  5.0   |\n",
      "| Infantino Wrap and Tie Bab... | I bought this carrier when... |  5.0   |\n",
      "| Evenflo 6 Pack Classic Gla... | It's always fun to write a... |  5.0   |\n",
      "| Graco FastAction Fold Jogg... | Graco's FastAction Jogging... |  5.0   |\n",
      "| Evenflo X Sport Plus Conve... | After seeing this in Paren... |  5.0   |\n",
      "| Britax 2012 B-Agile Stroll... | [I got this stroller for m... |  4.0   |\n",
      "| Ikea 36 Pcs Kalas Kids Pla... | For the price this set is ... |  5.0   |\n",
      "| Summer Infant Wide View Di... | I love this baby monitor. ... |  5.0   |\n",
      "| Phil &amp; Teds Navigator ... | I'm pretty happy with this... |  4.0   |\n",
      "+-------------------------------+-------------------------------+--------+\n",
      "+-------------------------------+-----------+------+\n",
      "|          review_clean         | sentiment | prob |\n",
      "+-------------------------------+-----------+------+\n",
      "| I absolutely love this pro... |     1     | 1.0  |\n",
      "| I am so HAPPY I brought th... |     1     | 1.0  |\n",
      "| My husband and I cannot st... |     1     | 1.0  |\n",
      "| Ive purchased both the PKo... |     1     | 1.0  |\n",
      "| We are big Best Bottoms fa... |     1     | 1.0  |\n",
      "| Amazing Love Love Love it ... |     1     | 1.0  |\n",
      "| After much research I purc... |     1     | 1.0  |\n",
      "| I researched a few differe... |     1     | 1.0  |\n",
      "| Great Pram RoccoI bought t... |     1     | 1.0  |\n",
      "| I just tried this hands fr... |     1     | 1.0  |\n",
      "| My husband and I assembled... |     1     | 1.0  |\n",
      "| I bought this seat for my ... |     1     | 1.0  |\n",
      "| I bought this carrier when... |     1     | 1.0  |\n",
      "| Its always fun to write a ... |     1     | 1.0  |\n",
      "| Gracos FastAction Jogging ... |     1     | 1.0  |\n",
      "| After seeing this in Paren... |     1     | 1.0  |\n",
      "| I got this stroller for my... |     1     | 1.0  |\n",
      "| For the price this set is ... |     1     | 1.0  |\n",
      "| I love this baby monitor  ... |     1     | 1.0  |\n",
      "| Im pretty happy with this ... |     1     | 1.0  |\n",
      "+-------------------------------+-----------+------+\n",
      "[33336 rows x 6 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data.sort('prob', ascending = False).print_rows(num_rows=20) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+-------------------------------+--------+\n",
      "|              name             |             review            | rating |\n",
      "+-------------------------------+-------------------------------+--------+\n",
      "| Fisher-Price Ocean Wonders... | We have not had ANY luck w... |  2.0   |\n",
      "| Levana Safe N'See Digital ... | This is the first review I... |  1.0   |\n",
      "| Safety 1st Exchangeable Ti... | I thought it sounded great... |  1.0   |\n",
      "| Adiri BPA Free Natural Nur... | I will try to write an obj... |  2.0   |\n",
      "| VTech Communications Safe ... | This is my second video mo... |  1.0   |\n",
      "| The First Years True Choic... | Note: we never installed b... |  1.0   |\n",
      "| Safety 1st High-Def Digita... | We bought this baby monito... |  1.0   |\n",
      "| Cloth Diaper Sprayer--styl... | I bought this sprayer out ... |  1.0   |\n",
      "| Philips AVENT Newborn Star... | It's 3am in the morning an... |  1.0   |\n",
      "| Motorola Digital Video Bab... | DO NOT BUY THIS BABY MONIT... |  1.0   |\n",
      "| Ellaroo Mei Tai Baby Carri... | This is basically an overp... |  1.0   |\n",
      "| Cosco Alpha Omega Elite Co... | I bought this car seat aft... |  1.0   |\n",
      "| Chicco Cortina KeyFit 30 T... | My wife and I have used th... |  1.0   |\n",
      "| Belkin WeMo Wi-Fi Baby Mon... | I read so many reviews say... |  2.0   |\n",
      "| Peg-Perego Tatamia High Ch... | I can see why there are so... |  2.0   |\n",
      "| NUK Cook-n-Blend Baby Food... | It thought this would be g... |  1.0   |\n",
      "| VTech Communications Safe ... | First, the distance on the... |  1.0   |\n",
      "| Safety 1st Deluxe 4-in-1 B... | This item is junk.  I orig... |  1.0   |\n",
      "| Regalo My Cot Portable Bed... | If I could give this produ... |  1.0   |\n",
      "| Thirsties Hemp Inserts 2 P... | My Experience: Babykicks I... |  5.0   |\n",
      "+-------------------------------+-------------------------------+--------+\n",
      "+-------------------------------+-----------+-------------------+\n",
      "|          review_clean         | sentiment |        prob       |\n",
      "+-------------------------------+-----------+-------------------+\n",
      "| We have not had ANY luck w... |     -1    | 8.47442279902e-16 |\n",
      "| This is the first review I... |     -1    | 1.59485709412e-15 |\n",
      "| I thought it sounded great... |     -1    | 8.14116655901e-14 |\n",
      "| I will try to write an obj... |     -1    | 9.83046180978e-14 |\n",
      "| This is my second video mo... |     -1    | 1.94179307181e-13 |\n",
      "| Note we never installed ba... |     -1    | 3.32465459763e-13 |\n",
      "| We bought this baby monito... |     -1    | 3.27225252468e-11 |\n",
      "| I bought this sprayer out ... |     -1    |  3.3295057511e-11 |\n",
      "| Its 3am in the morning and... |     -1    | 9.49459741846e-11 |\n",
      "| DO NOT BUY THIS BABY MONIT... |     -1    | 9.58560032751e-11 |\n",
      "| This is basically an overp... |     -1    | 4.35307210561e-10 |\n",
      "| I bought this car seat aft... |     -1    | 4.38585031197e-10 |\n",
      "| My wife and I have used th... |     -1    |  5.5878678999e-10 |\n",
      "| I read so many reviews say... |     -1    | 5.69148353565e-10 |\n",
      "| I can see why there are so... |     -1    |  5.8000514467e-10 |\n",
      "| It thought this would be g... |     -1    | 6.17540737178e-10 |\n",
      "| First the distance on thes... |     -1    |  8.0369335109e-10 |\n",
      "| This item is junk  I origi... |     -1    | 1.07681725421e-09 |\n",
      "| If I could give this produ... |     -1    | 1.59185859135e-09 |\n",
      "| My Experience Babykicks In... |     1     | 1.64259339725e-09 |\n",
      "+-------------------------------+-----------+-------------------+\n",
      "[33336 rows x 6 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data.sort('prob', ascending = True).print_rows(num_rows=20) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's compute the accuracy of the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy:  0.968504527193\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Accuracy on the training set:\n",
    "train_predictions = sentiment_model.predict(train_matrix)\n",
    "print 'training accuracy: ',(train_predictions == train_data['sentiment']).sum().astype(float)/train_data.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing accuracy:   0.932295416367\n"
     ]
    }
   ],
   "source": [
    "# Accuracy on the testing set: \n",
    "test_predictions =sentiment_model.predict(test_matrix)\n",
    "print 'testing accuracy:  ', (test_predictions == test_data['sentiment']).sum().astype(float)/test_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The model works pretty well.\n",
    "The model we used contains a lot of words and it takes some time to compute. How about we try to train the model with fewer words?\n",
    "\n",
    "Here is a list of 20 words given for this part of the assignment:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "significant_words = ['love', 'great', 'easy', 'old', 'little', 'perfect', 'loves', \n",
    "      'well', 'able', 'car', 'broke', 'less', 'even', 'waste', 'disappointed', \n",
    "      'work', 'product', 'money', 'would', 'return']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do word count again only for given vocabulary\n",
    "vectorizer_word_subset = CountVectorizer(vocabulary=significant_words)\n",
    "train_matrix_word_subset = vectorizer_word_subset.fit_transform(train_data['review_clean'])\n",
    "test_matrix_word_subset = vectorizer_word_subset.transform(test_data['review_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now let's train a new model:\n",
    "simple_model = LogisticRegression()\n",
    "\n",
    "simple_model = simple_model.fit(train_matrix_word_subset, train_data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+\n",
      "|   coefficient   |   word  |\n",
      "+-----------------+---------+\n",
      "|  1.36368975931  |   love  |\n",
      "|  0.943999590571 |  great  |\n",
      "|  1.19253827349  |   easy  |\n",
      "|  0.085512779463 |   old   |\n",
      "|  0.520185762718 |  little |\n",
      "|  1.50981247669  | perfect |\n",
      "|  1.67307389259  |  loves  |\n",
      "|  0.503760457767 |   well  |\n",
      "|  0.190908572065 |   able  |\n",
      "| 0.0588546711524 |   car   |\n",
      "+-----------------+---------+\n",
      "[20 rows x 2 columns]\n",
      "Note: Only the head of the SFrame is printed.\n",
      "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns.\n"
     ]
    }
   ],
   "source": [
    "# Let's have a look at the coefficients\n",
    "simple_model_coef_table = sframe.SFrame({'word':significant_words,\n",
    "                                         'coefficient':simple_model.coef_.flatten()})\n",
    "print simple_model_coef_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------+\n",
      "|   coefficient   |     word     |\n",
      "+-----------------+--------------+\n",
      "|  1.67307389259  |    loves     |\n",
      "|  1.50981247669  |   perfect    |\n",
      "|  1.36368975931  |     love     |\n",
      "|  1.19253827349  |     easy     |\n",
      "|  0.943999590571 |    great     |\n",
      "|  0.520185762718 |    little    |\n",
      "|  0.503760457767 |     well     |\n",
      "|  0.190908572065 |     able     |\n",
      "|  0.085512779463 |     old      |\n",
      "| 0.0588546711524 |     car      |\n",
      "| -0.209562864535 |     less     |\n",
      "| -0.320556236735 |   product    |\n",
      "| -0.362166742274 |    would     |\n",
      "| -0.511379631799 |     even     |\n",
      "| -0.621168773641 |     work     |\n",
      "| -0.898030737715 |    money     |\n",
      "|  -1.65157634497 |    broke     |\n",
      "|  -2.03369861394 |    waste     |\n",
      "|  -2.10933109032 |    return    |\n",
      "|  -2.3482982195  | disappointed |\n",
      "+-----------------+--------------+\n",
      "[20 rows x 2 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the words woth highest coefficient\n",
    "simple_model_coef_table.sort('coefficient', ascending = False).print_rows(num_rows=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy:  0.866822570007\n",
      "testing accuracy:   0.869360451164\n"
     ]
    }
   ],
   "source": [
    "# Accuracy on the training set:\n",
    "train_simple_predictions = simple_model.predict(train_matrix_word_subset)\n",
    "print 'training accuracy: ',(train_simple_predictions == train_data['sentiment']).sum().astype(float)/train_data.shape[0]\n",
    "\n",
    "# Accuracy on the testing set: \n",
    "\n",
    "test_simple_predictions = simple_model.predict(test_matrix_word_subset)\n",
    "print 'testing accuracy:  ',(test_simple_predictions == test_data['sentiment']).sum().astype(float)/test_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "The model with only 20 words performs decently on both training and testing data. Compared to the model with all the words, the difference in accuracies between the training and testing set is much smaller in the simple model with only 20 words. \n",
    "\n",
    "The first model characterise data better with more words, but in the mean time, we also overfitted the noise in the training data, therefore the testing accuracy is not as good ast the training accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline: Majority class prediction\n",
    "\n",
    "It is quite common to use the majority class classifier as the a baseline (or reference) model for comparison with your classifier model. The majority classifier model predicts the majority class for all data points. At the very least, you should healthily beat the majority class classifier, otherwise, the model is (usually) pointless.\n",
    "\n",
    "Let's look at the accuracy of a majority class classifier model on test data. Because the classifier will classifiy the majority class (pos in this case) right, and others wrong (neg in this case). We simply need to know (# of positive reviews)/(# of total reviews) in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.685565154787617"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(test_data['sentiment'].sum())/test_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, both of our classifiers work better than the majority class classifier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
